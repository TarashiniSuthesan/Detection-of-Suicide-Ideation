{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined-selftext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_join(df, sep, *cols):\n",
    "   ...:     from functools import reduce\n",
    "   ...:     return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), \n",
    "   ...:                   [df[col] for col in cols])\n",
    "   ...: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = str_join(df,\" \", 'title', 'usertext')\n",
    "del df['title']\n",
    "del df['usertext']\n",
    "df.rename(columns = {'y':'is_suicide'}, inplace = True)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "STOPWORDS = STOPWORDS.union(set(['im', 'ive', 'ill', 'wa', 'ha', 'aint', 'thats', 'la', 'le', 'please', 'feel', 'rly', 'u', 'nan', 'emptypost']))\n",
    "\n",
    "stop = STOPWORDS\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text_clean'], df['is_suicide'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(df['text'],\n",
    "                                   vector_size=300,\n",
    "                                   epochs=20,\n",
    "                                   window=10,\n",
    "                                   min_count=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-a03581770919>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
      "<ipython-input-10-a03581770919>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "words = set(w2v_model.wv.index_to_key)\n",
    "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train])\n",
    "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(300, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(300, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "Encoder = LabelEncoder()\n",
    "y_train = Encoder.fit_transform(y_train)\n",
    "y_test = Encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy Score: 65.33333333333333\n",
      "MultinomialNB Precision Score: 61.57635467980296\n",
      "MultinomialNB Recall Score: 70.62146892655367\n",
      "MultinomialNB F1-score Score: 65.78947368421053\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MNB = MultinomialNB()\n",
    "p = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "p = p.fit(X_train_vect_avg, y_train) \n",
    "predictions_MNB = p.predict(X_test_vect_avg)\n",
    "print(\"MultinomialNB Accuracy Score:\",accuracy_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB Precision Score:\",precision_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB Recall Score:\",recall_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB F1-score Score:\",f1_score(predictions_MNB, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.62      0.62       176\n",
      "           1       0.66      0.66      0.66       199\n",
      "\n",
      "    accuracy                           0.64       375\n",
      "   macro avg       0.64      0.64      0.64       375\n",
      "weighted avg       0.64      0.64      0.64       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_MNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score: 69.6\n",
      "SVM Precision Score: 70.44334975369459\n",
      "SVM Recall Score: 72.58883248730965\n",
      "SVM F1-score Score: 71.50000000000001\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine\n",
    "\n",
    "SVM = svm.SVC()\n",
    "SVM = SVM.fit(X_train_vect_avg, y_train)\n",
    "predictions_SVM = SVM.predict(X_test_vect_avg)\n",
    "print(\"SVM Accuracy Score:\",accuracy_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM Precision Score:\",precision_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM Recall Score:\",recall_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM F1-score Score:\",f1_score(predictions_SVM, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.61       176\n",
      "           1       0.66      0.73      0.69       199\n",
      "\n",
      "    accuracy                           0.66       375\n",
      "   macro avg       0.66      0.65      0.65       375\n",
      "weighted avg       0.66      0.66      0.66       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy Score: 66.93333333333334\n",
      "RF Precision Score: 66.99507389162561\n",
      "RF Recall Score: 70.46632124352331\n",
      "RF F1-score Score: 68.68686868686868\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "RF = RF.fit(X_train_vect_avg, y_train)\n",
    "predictions_RF = RF.predict(X_test_vect_avg)\n",
    "print(\"RF Accuracy Score:\",accuracy_score(predictions_RF, y_test)*100)\n",
    "print(\"RF Precision Score:\",precision_score(predictions_RF, y_test)*100)\n",
    "print(\"RF Recall Score:\",recall_score(predictions_RF, y_test)*100)\n",
    "print(\"RF F1-score Score:\",f1_score(predictions_RF, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.57      0.60       176\n",
      "           1       0.65      0.72      0.68       199\n",
      "\n",
      "    accuracy                           0.65       375\n",
      "   macro avg       0.65      0.64      0.64       375\n",
      "weighted avg       0.65      0.65      0.65       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score: 68.26666666666667\n",
      "Logistic Regression Precision Score: 69.95073891625616\n",
      "Logistic Regression Recall Score: 71.0\n",
      "Logistic Regression F1-score Score: 70.47146401985111\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR = LR.fit(X_train_vect_avg, y_train)\n",
    "predictions_LR = LR.predict(X_test_vect_avg)\n",
    "print(\"Logistic Regression Accuracy Score:\",accuracy_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression Precision Score:\",precision_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression Recall Score:\",recall_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression F1-score Score:\",f1_score(predictions_LR, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65       176\n",
      "           1       0.69      0.73      0.71       199\n",
      "\n",
      "    accuracy                           0.69       375\n",
      "   macro avg       0.68      0.68      0.68       375\n",
      "weighted avg       0.68      0.69      0.68       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy Score: 57.599999999999994\n",
      "Decision Tree Accuracy Score: 61.083743842364534\n",
      "Decision Tree Accuracy Score: 60.78431372549019\n",
      "Decision Tree Accuracy Score: 60.933660933660924\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train_vect_avg, y_train)\n",
    "predictions_DT = DT.predict(X_test_vect_avg)\n",
    "print(\"Decision Tree Accuracy Score:\",accuracy_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree Precision Score:\",precision_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree Recall Score:\",recall_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree F1-Score:\",f1_score(predictions_DT, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.57      0.57       176\n",
      "           1       0.62      0.63      0.63       199\n",
      "\n",
      "    accuracy                           0.60       375\n",
      "   macro avg       0.60      0.60      0.60       375\n",
      "weighted avg       0.60      0.60      0.60       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB Accuracy Score: 68.8\n",
      "LGB Precision Score: 73.39901477832512\n",
      "LGB Recall Score: 70.28301886792453\n",
      "LGB F1-score Score: 71.80722891566266\n"
     ]
    }
   ],
   "source": [
    "#Light Gradient Boosting\n",
    "\n",
    "import lightgbm as lgb\n",
    "LGB = lgb.LGBMClassifier()\n",
    "LGB.fit(X_train_vect_avg, y_train)\n",
    "predictions_LGB = LGB.predict(X_test_vect_avg)\n",
    "print(\"LGB Accuracy Score:\",accuracy_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB Precision Score:\",precision_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB Recall Score:\",recall_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB F1-score Score:\",f1_score(predictions_LGB, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.57      0.59       176\n",
      "           1       0.64      0.68      0.66       199\n",
      "\n",
      "    accuracy                           0.63       375\n",
      "   macro avg       0.63      0.63      0.63       375\n",
      "weighted avg       0.63      0.63      0.63       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions_LGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of Stacked model: 67.2\n",
      "precision score of Stacked model: 70.0\n",
      "recall score of Stacked model: 68.96551724137932\n",
      "f1 score of Stacked model: 69.4789081885856\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "clf_stack = StackingClassifier(classifiers =[LGB, XGB, RF, SVM], meta_classifier = LR)\n",
    "model_stack = clf_stack.fit(X_train_vect_avg, y_train)\n",
    "pred_stack = model_stack.predict(X_test_vect_avg)\t # predictions on test data using stacked model\n",
    "acc_stack = accuracy_score(y_test, pred_stack)\n",
    "precision_stack = precision_score(y_test, pred_stack)\n",
    "recall_stack = recall_score(y_test, pred_stack)\n",
    "f1_stack = f1_score(y_test, pred_stack) # evaluating accuracy\n",
    "print('accuracy score of Stacked model:', acc_stack * 100)\n",
    "print('precision score of Stacked model:', precision_stack * 100)\n",
    "print('recall score of Stacked model:', recall_stack * 100)\n",
    "print('f1 score of Stacked model:', f1_stack * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.57      0.60       176\n",
      "           1       0.65      0.72      0.69       199\n",
      "\n",
      "    accuracy                           0.65       375\n",
      "   macro avg       0.65      0.65      0.65       375\n",
      "weighted avg       0.65      0.65      0.65       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,pred_stack))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
