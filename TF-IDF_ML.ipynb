{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined-selftext.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_join(df, sep, *cols):\n",
    "   ...:     from functools import reduce\n",
    "   ...:     return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), \n",
    "   ...:                   [df[col] for col in cols])\n",
    "   ...: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = str_join(df,\" \", 'title', 'usertext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "STOPWORDS = STOPWORDS.union(set(['im', 'ive', 'ill', 'wa', 'ha', 'aint', 'thats', 'la', 'le', 'please', 'feel', 'rly', 'u', 'nan', 'emptypost']))\n",
    "\n",
    "stop = STOPWORDS\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['title']\n",
    "del df['usertext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "      <th>text_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[need, help, hi, know, phrase, situation, try,...</td>\n",
       "      <td>need help hi know phrase situation try life go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[feeling, overwhelmed, hopeless, depressed, pa...</td>\n",
       "      <td>feeling overwhelmed hopeless depressed past co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[matter, anymore, getting, worse, hi, know, de...</td>\n",
       "      <td>matter anymore getting worse hi know devastate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[tired, hearing, bullshit, shit, like, better,...</td>\n",
       "      <td>tired hearing bullshit shit like better purpos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[wish, wish, prettier, wish, like, burden, wis...</td>\n",
       "      <td>wish wish prettier wish like burden wish broke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y                                               text  \\\n",
       "0  0  [need, help, hi, know, phrase, situation, try,...   \n",
       "1  1  [feeling, overwhelmed, hopeless, depressed, pa...   \n",
       "2  0  [matter, anymore, getting, worse, hi, know, de...   \n",
       "3  1  [tired, hearing, bullshit, shit, like, better,...   \n",
       "4  0  [wish, wish, prettier, wish, like, burden, wis...   \n",
       "\n",
       "                                       text_original  \n",
       "0  need help hi know phrase situation try life go...  \n",
       "1  feeling overwhelmed hopeless depressed past co...  \n",
       "2  matter anymore getting worse hi know devastate...  \n",
       "3  tired hearing bullshit shit like better purpos...  \n",
       "4  wish wish prettier wish like burden wish broke...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].dropna(inplace=True)\n",
    "# 2. Changing all text to lowercase\n",
    "df['text_original'] = df['text']\n",
    "df['text'] = [entry.lower() for entry in df['text']]\n",
    "# 3. Tokenization-In this each entry in the corpus will be broken into set of words\n",
    "df['text']= [word_tokenize(entry) for entry in df['text']]\n",
    "# 4. Remove Stop words, Non-Numeric and perfoming Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,entry in enumerate(df['text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    df.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(df['text_final'],df['y'],test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "y_train = Encoder.fit_transform(y_train)\n",
    "y_test = Encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer()\n",
    "Tfidf_vect.fit(df['text_final'])\n",
    "X_train = Tfidf_vect.transform(X_train)\n",
    "X_test = Tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.48      0.59       198\n",
      "           1       0.59      0.84      0.69       177\n",
      "\n",
      "    accuracy                           0.65       375\n",
      "   macro avg       0.68      0.66      0.64       375\n",
      "weighted avg       0.69      0.65      0.64       375\n",
      "\n",
      "MultinomialNB Accuracy Score: 65.06666666666666\n",
      "MultinomialNB Precision Score: 84.18079096045197\n",
      "MultinomialNB Recall Score: 59.12698412698413\n",
      "MultinomialNB F1-score Score: 69.46386946386947\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "\n",
    "MNB = naive_bayes.MultinomialNB()\n",
    "MNB.fit(X_train, y_train)\n",
    "predictions_MNB = MNB.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_MNB))\n",
    "print(\"MultinomialNB Accuracy Score:\",accuracy_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB Precision Score:\",precision_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB Recall Score:\",recall_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB F1-score Score:\",f1_score(predictions_MNB, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.64      0.69       198\n",
      "           1       0.65      0.76      0.70       177\n",
      "\n",
      "    accuracy                           0.69       375\n",
      "   macro avg       0.70      0.70      0.69       375\n",
      "weighted avg       0.70      0.69      0.69       375\n",
      "\n",
      "SVM Accuracy Score: 69.33333333333334\n",
      "SVM Precision Score: 75.70621468926554\n",
      "SVM Recall Score: 65.0485436893204\n",
      "SVM F1-score Score: 69.9738903394256\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine\n",
    "\n",
    "from sklearn import svm\n",
    "SVM = svm.SVC()\n",
    "SVM.fit(X_train, y_train)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_SVM))\n",
    "print(\"SVM Accuracy Score:\",accuracy_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM Precision Score:\",precision_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM Recall Score:\",recall_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM F1-score Score:\",f1_score(predictions_SVM, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.57      0.63       198\n",
      "           1       0.60      0.74      0.66       177\n",
      "\n",
      "    accuracy                           0.65       375\n",
      "   macro avg       0.66      0.65      0.65       375\n",
      "weighted avg       0.66      0.65      0.65       375\n",
      "\n",
      "RF Accuracy Score: 64.8\n",
      "RF Precision Score: 74.01129943502825\n",
      "RF Recall Score: 60.36866359447005\n",
      "RF F1-score Score: 66.49746192893402\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)\n",
    "predictions_RF = RF.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_RF))\n",
    "print(\"RF Accuracy Score:\",accuracy_score(predictions_RF, y_test)*100)\n",
    "print(\"RF Precision Score:\",precision_score(predictions_RF, y_test)*100)\n",
    "print(\"RF Recall Score:\",recall_score(predictions_RF, y_test)*100)\n",
    "print(\"RF F1-score Score:\",f1_score(predictions_RF, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.68       198\n",
      "           1       0.65      0.75      0.70       177\n",
      "\n",
      "    accuracy                           0.69       375\n",
      "   macro avg       0.69      0.69      0.69       375\n",
      "weighted avg       0.70      0.69      0.69       375\n",
      "\n",
      "Logistic Regression Accuracy Score: 69.06666666666666\n",
      "Logistic Regression Precision Score: 75.14124293785311\n",
      "Logistic Regression Recall Score: 64.8780487804878\n",
      "Logistic Regression F1-score Score: 69.63350785340315\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "predictions_LR = LR.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_LR))\n",
    "print(\"Logistic Regression Accuracy Score:\",accuracy_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression Precision Score:\",precision_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression Recall Score:\",recall_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression F1-score Score:\",f1_score(predictions_LR, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64       198\n",
      "           1       0.60      0.60      0.60       177\n",
      "\n",
      "    accuracy                           0.62       375\n",
      "   macro avg       0.62      0.62      0.62       375\n",
      "weighted avg       0.62      0.62      0.62       375\n",
      "\n",
      "Decision Tree Accuracy Score: 62.133333333333326\n",
      "Decision Tree Accuracy Score: 60.451977401129945\n",
      "Decision Tree Accuracy Score: 59.77653631284916\n",
      "Decision Tree Accuracy Score: 60.1123595505618\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)\n",
    "predictions_DT = DT.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_DT))\n",
    "print(\"Decision Tree Accuracy Score:\",accuracy_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree Precision Score:\",precision_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree Recall Score:\",recall_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree F1-Score:\",f1_score(predictions_DT, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.63      0.67       198\n",
      "           1       0.64      0.74      0.69       177\n",
      "\n",
      "    accuracy                           0.68       375\n",
      "   macro avg       0.68      0.68      0.68       375\n",
      "weighted avg       0.69      0.68      0.68       375\n",
      "\n",
      "LGB Accuracy Score: 68.0\n",
      "LGB Precision Score: 74.01129943502825\n",
      "LGB Recall Score: 63.90243902439025\n",
      "LGB F1-score Score: 68.58638743455498\n"
     ]
    }
   ],
   "source": [
    "#Light Gradient Boosting\n",
    "\n",
    "import lightgbm as lgb\n",
    "LGB = lgb.LGBMClassifier()\n",
    "LGB.fit(X_train, y_train)\n",
    "predictions_LGB = LGB.predict(X_test)\n",
    "\n",
    "print(\"LGB Accuracy Score:\",accuracy_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB Precision Score:\",precision_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB Recall Score:\",recall_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB F1-score Score:\",f1_score(predictions_LGB, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of Stacked model: 68.8\n",
      "precision score of Stacked model: 64.28571428571429\n",
      "recall score of Stacked model: 76.27118644067797\n",
      "f1 score of Stacked model: 69.76744186046511\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.62      0.68       198\n",
      "           1       0.64      0.76      0.70       177\n",
      "\n",
      "    accuracy                           0.69       375\n",
      "   macro avg       0.69      0.69      0.69       375\n",
      "weighted avg       0.70      0.69      0.69       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "clf_stack = StackingClassifier(classifiers =[LGB, XGB, RF, SVM], meta_classifier = LR)\n",
    "model_stack = clf_stack.fit(X_train, y_train) # training of stacked model\n",
    "pred_stack = model_stack.predict(X_test)\t # predictions on test data using stacked model\n",
    "acc_stack = accuracy_score(y_test, pred_stack)\n",
    "precision_stack = precision_score(y_test, pred_stack)\n",
    "recall_stack = recall_score(y_test, pred_stack)\n",
    "f1_stack = f1_score(y_test, pred_stack)# evaluating accuracy\n",
    "print('accuracy score of Stacked model:', acc_stack * 100)\n",
    "print('precision score of Stacked model:', precision_stack * 100)\n",
    "print('recall score of Stacked model:', recall_stack * 100)\n",
    "print('f1 score of Stacked model:', f1_stack * 100)\n",
    "print(classification_report(y_test,pred_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
