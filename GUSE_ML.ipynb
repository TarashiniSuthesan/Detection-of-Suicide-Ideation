{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined-selftext.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_join(df, sep, *cols):\n",
    "   ...:     from functools import reduce\n",
    "   ...:     return reduce(lambda x, y: x.astype(str).str.cat(y.astype(str), sep=sep), \n",
    "   ...:                   [df[col] for col in cols])\n",
    "   ...: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = str_join(df,\" \", 'title', 'usertext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "STOPWORDS = STOPWORDS.union(set(['im', 'ive', 'ill', 'wa', 'ha', 'aint', 'thats', 'la', 'le', 'please', 'feel', 'rly', 'u', 'nan', 'emptypost']))\n",
    "\n",
    "stop = STOPWORDS\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['title']\n",
    "del df['usertext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>need help hi know phrase situation try life go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>feeling overwhelmed hopeless depressed past co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>matter anymore getting worse hi know devastate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>tired hearing bullshit shit like better purpos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>wish wish prettier wish like burden wish broke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y                                               text\n",
       "0  0  need help hi know phrase situation try life go...\n",
       "1  1  feeling overwhelmed hopeless depressed past co...\n",
       "2  0  matter anymore getting worse hi know devastate...\n",
       "3  1  tired hearing bullshit shit like better purpos...\n",
       "4  0  wish wish prettier wish like burden wish broke..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_suicide\"] = df[\"y\"].apply(lambda x: \"depressed\" if x < 1 else \"suicidal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicidal_reddits = df[df.is_suicide == \"suicidal\"]\n",
    "depressed_reddits = df[df.is_suicide == \"depressed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suicidal_df = suicidal_reddits.sample(n=len(depressed_reddits), random_state=RANDOM_SEED)\n",
    "suicidal_df = suicidal_reddits\n",
    "depressed_df = depressed_reddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddits_df = (pd.concat([suicidal_df, depressed_df]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reddits, test_reddits, y_train, y_test =\\\n",
    "  train_test_split(\n",
    "    reddits_df.text, \n",
    "    reddits_df.is_suicide, \n",
    "    test_size=.2, \n",
    "    random_state=42\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1498/1498 [00:17<00:00, 83.49it/s] \n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for r in tqdm(train_reddits):\n",
    "  emb = use(r)\n",
    "  reddit_emb = tf.reshape(emb, [-1]).numpy()\n",
    "  X_train.append(reddit_emb)\n",
    "\n",
    "X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:04<00:00, 81.85it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for r in tqdm(test_reddits):\n",
    "  emb = use(r)\n",
    "  reddit_emb = tf.reshape(emb, [-1]).numpy()\n",
    "  X_test.append(reddit_emb)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "Encoder = LabelEncoder()\n",
    "y_train = Encoder.fit_transform(y_train)\n",
    "y_test = Encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1498, 512) (375, 512)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69       182\n",
      "           1       0.71      0.67      0.69       193\n",
      "\n",
      "    accuracy                           0.69       375\n",
      "   macro avg       0.69      0.69      0.69       375\n",
      "weighted avg       0.69      0.69      0.69       375\n",
      "\n",
      "MultinomialNB Accuracy Score: 68.8\n",
      "MultinomialNB Precision Score: 66.83937823834198\n",
      "MultinomialNB Recall Score: 70.87912087912088\n",
      "MultinomialNB F1-score Score: 68.80000000000001\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "MNB = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "MNB.fit(X_train, y_train)\n",
    "predictions_MNB = MNB.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_MNB))\n",
    "print(\"MultinomialNB Accuracy Score:\",accuracy_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB Precision Score:\",precision_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB Recall Score:\",recall_score(predictions_MNB, y_test)*100)\n",
    "print(\"MultinomialNB F1-score Score:\",f1_score(predictions_MNB, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.65      0.68       182\n",
      "           1       0.70      0.75      0.72       193\n",
      "\n",
      "    accuracy                           0.70       375\n",
      "   macro avg       0.70      0.70      0.70       375\n",
      "weighted avg       0.70      0.70      0.70       375\n",
      "\n",
      "SVM Accuracy Score: 70.39999999999999\n",
      "SVM Precision Score: 75.12953367875647\n",
      "SVM Recall Score: 69.71153846153845\n",
      "SVM F1-score Score: 72.31920199501246\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine\n",
    "\n",
    "from sklearn import svm\n",
    "SVM = svm.SVC()\n",
    "SVM.fit(X_train, y_train)\n",
    "predictions_SVM = SVM.predict(X_test)\n",
    "\n",
    "#print(\"SVM Accuracy Score:\",accuracy_score(predictions_SVM, y_test)*100)\n",
    "print(classification_report(y_test, predictions_SVM))\n",
    "print(\"SVM Accuracy Score:\",accuracy_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM Precision Score:\",precision_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM Recall Score:\",recall_score(predictions_SVM, y_test)*100)\n",
    "print(\"SVM F1-score Score:\",f1_score(predictions_SVM, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       182\n",
      "           1       0.70      0.71      0.70       193\n",
      "\n",
      "    accuracy                           0.69       375\n",
      "   macro avg       0.69      0.69      0.69       375\n",
      "weighted avg       0.69      0.69      0.69       375\n",
      "\n",
      "RF Accuracy Score: 69.06666666666666\n",
      "RF Precision Score: 70.98445595854922\n",
      "RF Recall Score: 69.54314720812182\n",
      "RF F1-score Score: 70.25641025641026\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)\n",
    "predictions_RF = RF.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_RF))\n",
    "print(\"RF Accuracy Score:\",accuracy_score(predictions_RF, y_test)*100)\n",
    "print(\"RF Precision Score:\",precision_score(predictions_RF, y_test)*100)\n",
    "print(\"RF Recall Score:\",recall_score(predictions_RF, y_test)*100)\n",
    "print(\"RF F1-score Score:\",f1_score(predictions_RF, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70       182\n",
      "           1       0.72      0.74      0.73       193\n",
      "\n",
      "    accuracy                           0.71       375\n",
      "   macro avg       0.71      0.71      0.71       375\n",
      "weighted avg       0.71      0.71      0.71       375\n",
      "\n",
      "Logistic Regression Accuracy Score: 71.46666666666667\n",
      "Logistic Regression Precision Score: 73.57512953367875\n",
      "Logistic Regression Recall Score: 71.71717171717171\n",
      "Logistic Regression F1-score Score: 72.63427109974423\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "predictions_LR = LR.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_LR))\n",
    "print(\"Logistic Regression Accuracy Score:\",accuracy_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression Precision Score:\",precision_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression Recall Score:\",recall_score(predictions_LR, y_test)*100)\n",
    "print(\"Logistic Regression F1-score Score:\",f1_score(predictions_LR, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.51      0.53       182\n",
      "           1       0.57      0.62      0.59       193\n",
      "\n",
      "    accuracy                           0.57       375\n",
      "   macro avg       0.56      0.56      0.56       375\n",
      "weighted avg       0.56      0.57      0.56       375\n",
      "\n",
      "Decision Tree Accuracy Score: 56.53333333333334\n",
      "Decision Tree Precision Score: 61.6580310880829\n",
      "Decision Tree Recall Score: 57.21153846153846\n",
      "Decision Tree f1 Score: 59.35162094763092\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)\n",
    "predictions_DT = DT.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_DT))\n",
    "print(\"Decision Tree Accuracy Score:\",accuracy_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree Precision Score:\",precision_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree Recall Score:\",recall_score(predictions_DT, y_test)*100)\n",
    "print(\"Decision Tree f1 Score:\",f1_score(predictions_DT, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:366: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.63      0.66       182\n",
      "           1       0.67      0.73      0.70       193\n",
      "\n",
      "    accuracy                           0.68       375\n",
      "   macro avg       0.68      0.68      0.68       375\n",
      "weighted avg       0.68      0.68      0.68       375\n",
      "\n",
      "LGB Accuracy Score: 68.0\n",
      "LGB Precision Score: 73.05699481865285\n",
      "LGB Recall Score: 67.46411483253588\n",
      "LGB F1-score Score: 70.1492537313433\n"
     ]
    }
   ],
   "source": [
    "#Light Gradient Boosting\n",
    "\n",
    "!pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "LGB = lgb.LGBMClassifier()\n",
    "LGB.fit(X_train, y_train)\n",
    "predictions_LGB = LGB.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions_LGB))\n",
    "print(\"LGB Accuracy Score:\",accuracy_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB Precision Score:\",precision_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB Recall Score:\",recall_score(predictions_LGB, y_test)*100)\n",
    "print(\"LGB F1-score Score:\",f1_score(predictions_LGB, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score of Stacked model: 68.53333333333333\n",
      "precision score of Stacked model: 68.29268292682927\n",
      "recall score of Stacked model: 72.53886010362694\n",
      "f1 score of Stacked model: 70.35175879396985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66       182\n",
      "           1       0.68      0.73      0.70       193\n",
      "\n",
      "    accuracy                           0.69       375\n",
      "   macro avg       0.69      0.68      0.68       375\n",
      "weighted avg       0.69      0.69      0.68       375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "clf_stack = StackingClassifier(classifiers =[LGB, XGB, RF, SVM], meta_classifier = LR)\n",
    "model_stack = clf_stack.fit(X_train, y_train)\n",
    "pred_stack = model_stack.predict(X_test)\n",
    "\n",
    "acc_stack = accuracy_score(y_test, pred_stack)\n",
    "precision_stack = precision_score(y_test, pred_stack)\n",
    "recall_stack = recall_score(y_test, pred_stack)\n",
    "f1_stack = f1_score(y_test, pred_stack)# evaluating accuracy\n",
    "print('accuracy score of Stacked model:', acc_stack * 100)\n",
    "print('precision score of Stacked model:', precision_stack * 100)\n",
    "print('recall score of Stacked model:', recall_stack * 100)\n",
    "print('f1 score of Stacked model:', f1_stack * 100)\n",
    "print(classification_report(y_test,pred_stack))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
